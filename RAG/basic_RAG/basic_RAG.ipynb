{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval from Vector Database and Basic Question Answering  \n",
    "Starts the same as basic_retrieval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ../.rag_venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from VectorDatabase import VectorDatabase\n",
    "from Document       import Document\n",
    "from os             import listdir\n",
    "from os.path        import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise vector database\n",
    "v_db = VectorDatabase()\n",
    "v_db.setClient(persistent=False)\n",
    "v_db.createCollection(\"Bible_John\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents into vector database\n",
    "dataset_folder = \"documents\"\n",
    "\n",
    "def check_is_txt(file):\n",
    "    if (isfile(join(dataset_folder, file))) and (file.endswith('.txt')):\n",
    "        return True\n",
    "\n",
    "metadatas = [{\"filepath\": join(dataset_folder,f), \n",
    "              \"book\"    :(f.split('_')[0]), \n",
    "              \"chapter\" :(f.split('_')[1].split('.')[0])\n",
    "              } for f in listdir(dataset_folder) if check_is_txt(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filepath': 'documents/john_3.txt', 'book': 'john', 'chapter': '3'},\n",
       " {'filepath': 'documents/john_2.txt', 'book': 'john', 'chapter': '2'},\n",
       " {'filepath': 'documents/john_1.txt', 'book': 'john', 'chapter': '1'},\n",
       " {'filepath': 'documents/john_5.txt', 'book': 'john', 'chapter': '5'},\n",
       " {'filepath': 'documents/john_4.txt', 'book': 'john', 'chapter': '4'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise lists to be added to collection\n",
    "doc_list     = []\n",
    "doc_metadata = []\n",
    "embeddings   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in each dataset file\n",
    "for item in metadatas:\n",
    "    path = item[\"filepath\"]\n",
    "\n",
    "    doc  = Document(item[\"filepath\"])\n",
    "\n",
    "    doc_list.append(doc.fulltext)\n",
    "    doc_metadata.append(item)\n",
    "    embeddings.append(doc.embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_list) == len(doc_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to collection\n",
    "v_db.addToCollection(docs   = doc_list, \n",
    "                     embeds = embeddings,\n",
    "                     meta   = doc_metadata, \n",
    "                     ids    = [f\"id{i}\" for i in range(len(doc_list))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama  \n",
    "#### Download Model with Ollama\n",
    "Install Ollama: https://ollama.com  \n",
    "Set directory to save models in: https://dev.to/hamed0406/how-to-change-place-of-saving-models-on-ollama-4ko8  \n",
    "Download a model (be wary of model size)  \n",
    "\n",
    "I will use Deepseek-R1:1.5b (1.1GB) found here: https://ollama.com/library/deepseek-r1:1.5b\n",
    "\n",
    "#### Serve Ollama\n",
    "optional:   \n",
    "export OLLAMA_HOST=127.0.0.1 # environment variable to set ollama host  \n",
    "export OLLAMA_PORT=11434     # environment variable to set the ollama port  \n",
    "\n",
    "start serving ollama:  \n",
    "ollama serve  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-r1:1.5b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema            import BaseRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts           import PromptTemplate\n",
    "from langchain_community.llms    import Ollama\n",
    "from langchain.chains            import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow custom retrieval function\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, retrieval_function, num_docs_to_retrieve):\n",
    "        super().__init__()\n",
    "        self._retrieval_function   = retrieval_function\n",
    "        self._num_docs_to_retrieve = num_docs_to_retrieve\n",
    "        \n",
    "    def _get_relevant_documents(self, query: str) -> list:\n",
    "        retrieved      = self._retrieval_function(query, self._num_docs_to_retrieve)\n",
    "        num_of_results = len(retrieved['ids'][0])\n",
    "        results        = []\n",
    "        for i in range(num_of_results):\n",
    "            results.append(Document(\n",
    "                                page_content = retrieved['documents'][0][i],\n",
    "                                metadata     = retrieved['metadatas'][0][i]\n",
    "                                ))\n",
    "        return results\n",
    "\n",
    "# main pipeline\n",
    "def RAG_pipeline(model_name, retriever_func, num_docs):\n",
    "    # Custom prompt template\n",
    "    prompt_template = \"\"\"\n",
    "    Use only the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer based only on the context, just say you don't know.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    retriever = CustomRetriever(retriever_func, num_docs)\n",
    "    llm       = Ollama(model=model_name)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/x9tnwk2519v14dprnysc0tpw0000gn/T/ipykernel_13407/4283359118.py:38: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm       = Ollama(model=model_name)\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RAG_pipeline(model_name, v_db.queryDatabase, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain.invoke(\"Who is Jesus?\") # response takes ~1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model Output *****\n",
      "<think>\n",
      "Okay, so I'm trying to figure out who Jesus is. From what I've heard, there are different people mentioned in this text, but the question is asking specifically about Jesus. Let me go through each part and see how that makes sense.\n",
      "\n",
      "First, there's a man named Jonathan who tells Jesus to be the Messiah. That seems like an important clue because the Messiah was Jesus. Then, later on, after some time, he reappears with a woman talking to him. That woman's testimony about him being the Messiah is key because it leads many Samaritans to believe in him.\n",
      "\n",
      "Wait, there are also people from Galilee and other towns who heard about his story. They saw him visit Cana and turn water into wine, which is Jesus' trick to make people believe. Then he heals an official's son in Capernaum who was sick, which must have made the people believe that he was trustworthy.\n",
      "\n",
      "So putting it all together, the woman's testimony about how he told everything she ever did seems like a strong message. Also, the fact that after leaving for Galilee, he returned to see the people and start healing someone makes sense because that would show people that Jesus was good and could do big things. The water trick and making the son live again also showed his authority.\n",
      "\n",
      "I think it's important that the woman called him the Messiah because her words were so specific and convincing. Other people from different places heard about his story, saw signs of his magic (like turning water into wine), and believed based on his actions. So Jesus is someone who was revealed to be a good leader in taking people's trust through various signs and his actions.\n",
      "\n",
      "I should make sure I'm not mixing up any other people here. The key points are the woman's testimony, the signs he used (like turning water into wine), and showing trustworthiness by healing the sick. These all point towards him being Jesus.\n",
      "</think>\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Jesus is the Christ, also known as the Messiah, who is revealed to be a leader of good deeds through several key elements in the text:\n",
      "\n",
      "1. **Woman's Testimony**: The woman heard that Jesus had told everything she ever did and called him the Messiah. This specific and convincing testimony becomes a significant message for many Samaritans.\n",
      "\n",
      "2. **Signs of His Magic**: Later, when visiting Galilee, he used the trick of turning water into wine, which became seen as a sign of his authority and good character.\n",
      "\n",
      "3. **Healing an Official's Son**: He healed someone's sick son in Capernaum by saying that their son would live, demonstrating his trustworthy nature.\n",
      "\n",
      "4. **Return to Galilee**: After leaving for Galilee, he returned to show trustworthiness (healing the sick) and to others who heard about his story, making them believe him.\n",
      "\n",
      "In conclusion, Jesus is the Christ, identified through his ability to make people trust him with signs, his actions that demonstrated his good reputation, and the woman's specific testimony that led many Samaritans to believe.\n",
      "\n",
      "***** Source(s) *****\n",
      "{'book': 'john', 'chapter': '3', 'filepath': 'documents/john_3.txt'}\n",
      "{'book': 'john', 'chapter': '4', 'filepath': 'documents/john_4.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Model Output *****\")\n",
    "print(response['result'])\n",
    "\n",
    "print(\"\\n***** Source(s) *****\")\n",
    "for source in response['source_documents']:\n",
    "    print(source.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not entirely theologically correct, but also not too bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(message, history):\n",
    "    response = qa_chain.invoke(message)\n",
    "    result = response['result']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(\n",
    "    fn=invoke_model, \n",
    "    type=\"messages\"\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
